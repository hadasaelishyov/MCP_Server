# ğŸ§ª Pytest Generator MCP Server

> **Evidence-first test generation for Python** â€” Analyze â†’ Generate â†’ Run â†’ Fix  
> Built as an **MCP (Model Context Protocol) server** for coding agents (Claude Desktop / MCP Inspector and other MCP clients).

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/downloads/)
[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-brightgreen.svg)](https://modelcontextprotocol.io/)
[![Tests](https://img.shields.io/badge/tests-300%2B-green.svg)](#-testing)
[![Coverage](https://img.shields.io/badge/coverage-~77%25-yellowgreen.svg)](#-testing)
[![License: MIT](https://img.shields.io/badge/license-MIT-yellow.svg)](LICENSE)

---

## âœ¨ What It Does

Pytest Generator MCP is a complete test automation pipeline exposed as MCP tools. It analyzes Python code, generates runnable pytest test suites, executes them in an isolated environment, and (optionally) proposes fixes based on failures.

### Core workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CORE TOOLS                             â”‚
â”‚                                                              â”‚
â”‚  ğŸ“„ Source Code                                               â”‚
â”‚     â†“                                                        â”‚
â”‚  ğŸ” analyze_code     â†’ AST structure, complexity, warnings    â”‚
â”‚     â†“                                                        â”‚
â”‚  ğŸ§ª generate_tests   â†’ Templates + evidence-based assertions  â”‚
â”‚     â†“                                                        â”‚
â”‚  â–¶ï¸  run_tests       â†’ Pass/fail + coverage report            â”‚
â”‚     â†“ (optional)                                            â”‚
â”‚  ğŸ› ï¸  fix_code        â†’ AI-assisted fixes + verification       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optional GitHub workflow (plugin)

When enabled with a `GITHUB_TOKEN`, the server can integrate with a GitHub development workflow: analyze a repository, generate tests for selected files, and optionally create pull requests and comment results.

---

## ğŸš€ Features

### ğŸ” Code Analysis (`analyze_code`)
- AST-based parsing of Python code
- Extracts functions, classes, methods, and parameters (including `positional_only`, `keyword_only`, `*args`, `**kwargs`)
- Calculates cyclomatic complexity and basic statistics
- Reports type-hint coverage and warns about missing hints / high complexity
- Designed to be deterministic and safe (static analysis only)

### ğŸ§ª Test Generation (`generate_tests`)

**Layer 1 â€” Template Tests**
- Generates runnable smoke tests for each function
- Generates class instantiation tests
- Generates method invocation tests for class methods

**Layer 2 â€” Evidence-Based Enrichment**
- ğŸ“ **Doctest Extraction** â€” converts docstring examples to real `assert` statements
- ğŸ·ï¸ **Type Assertions** â€” safe `isinstance` checks derived from type annotations (including Optionals/Unions)
- âš ï¸ **Exception Detection** â€” generates `pytest.raises(...)` tests inferred from AST `raise` statements
- ğŸ“Š **Boundary Values** â€” edge cases such as `0/-1/""/[]/{}` and other type-shaped boundaries
- ğŸ”¤ **Naming Heuristics** â€” adds hints for `is_*` and `get_*` style functions

**Layer 3 â€” AI Enhancement (Optional)**
- Strengthens weak assertions when static evidence is insufficient
- Improves exception trigger conditions and adds additional edge cases
- Cleanly falls back to template/evidence tests if AI is unavailable

> **Design rule:** *Never guess expected values without evidence.*  
> Wrong tests are worse than weak tests â€” templates + evidence are the guardrails, AI is an optional booster.

### â–¶ï¸ Test Execution (`run_tests`)
- Runs tests inside an isolated temporary directory
- Returns a structured summary (passed/failed totals + failure details)
- Supports coverage reporting using `pytest-cov`
- Auto-detects module name and imports to keep generated tests runnable

### ğŸ› ï¸ Code Fixing (`fix_code`) (Optional)
- Parses pytest output and identifies failing areas
- Uses AI (when enabled) to propose patches to the source code
- Can verify fixes by re-running tests and returning a verification summary

---

## ğŸ”— GitHub Tools (Optional Plugin)

GitHub tools are available only when `GITHUB_TOKEN` is provided (recommended to keep them optional and disabled by default).

### ğŸ” `analyze_repository`
- Clones a repository to a temporary folder
- Analyzes Python files (optionally with path filters)
- Identifies files likely needing tests and returns a structured report

### ğŸ§· `create_test_pr`
- Creates a new branch
- Writes/updates generated tests under `tests/`
- Opens a pull request with a clean description

### ğŸ’¬ `comment_test_results`
- Adds test results and coverage summary as a PR comment

> Recommended usage: for trusted repos and demo workflows. Network/API failures should be handled gracefully.

---

## ğŸ“¦ Installation

### Requirements
- Python 3.10+
- `uv` recommended (or `pip`)

### Setup

```bash
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>

# Install dependencies
uv sync

# Run tests
uv run pytest -q
```

If you prefer pip:

```bash
pip install -e .
pytest -q
```

---

## ğŸ§  Run as an MCP Server

### Using MCP Inspector

```bash
npx @modelcontextprotocol/inspector .venv/Scripts/python.exe -m src.server
```

### Using Claude Desktop (Windows example)

Edit:
`%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "pytest-generator": {
      "command": "C:\\path\\to\\project\\.venv\\Scripts\\python.exe",
      "args": ["-m", "src.server"],
      "cwd": "C:\\path\\to\\project",
      "env": {
        "OPENAI_API_KEY": "sk-optional",
        "GITHUB_TOKEN": "ghp-optional"
      }
    }
  }
}
```

Notes:
- `OPENAI_API_KEY` is optional (only needed for AI enhancement and AI fixing).
- `GITHUB_TOKEN` is optional (only needed for GitHub integration tools).

---

## ğŸ¯ Tool Reference

### Core Tools

| Tool | Purpose |
|------|---------|
| `analyze_code` | Parse and extract structure + warnings |
| `generate_tests` | Generate pytest suite (templates + evidence + optional AI) |
| `run_tests` | Execute tests and report pass/fail + coverage |
| `fix_code` | Optional AI fixer for failures, with verification |

### GitHub Tools (Optional)

| Tool | Purpose |
|------|---------|
| `analyze_repository` | Clone and analyze repo, find files needing tests |
| `create_test_pr` | Create PR that adds/updates tests |
| `comment_test_results` | Comment results and coverage on PR |

---

## ğŸ“– Examples

### Example: Input Code

```python
def calculate_discount(price: float, percentage: float) -> float:
    """Calculate discounted price.

    >>> calculate_discount(100.0, 10.0)
    90.0
    """
    if percentage < 0 or percentage > 100:
        raise ValueError("Percentage must be between 0 and 100")
    return price * (1 - percentage / 100)
```

### Example: Generated Tests (evidence-based)

```python
import pytest

from your_module import calculate_discount

def test_calculate_discount_doctest_1():
    assert calculate_discount(100.0, 10.0) == 90.0

def test_calculate_discount_raises_valueerror():
    with pytest.raises(ValueError, match="Percentage must be between 0 and 100"):
        calculate_discount(100.0, -1.0)
    with pytest.raises(ValueError, match="Percentage must be between 0 and 100"):
        calculate_discount(100.0, 101.0)

def test_calculate_discount_boundary_values():
    assert calculate_discount(100.0, 0.0) == 100.0
    assert calculate_discount(100.0, 100.0) == 0.0
```

### Example: Test Execution Output (summarized)

```
ğŸ§ª TEST EXECUTION RESULTS
==================================================

âœ… All tests passed!

ğŸ“Š Summary:
  â€¢ Total:  3
  â€¢ Passed: 3
  â€¢ Failed: 0

ğŸ“ˆ Code Coverage:
  â€¢ Coverage: 100.0%
```

---

## ğŸ—ï¸ Architecture

The server is intentionally thin: it only handles MCP protocol wiring.  
Business logic lives in services and tool modules for testability and separation of concerns.

```
src/
â”œâ”€â”€ server.py                      # MCP server entry point
â”œâ”€â”€ constants.py
â”œâ”€â”€ services/                      # Orchestration + shared patterns
â”‚   â”œâ”€â”€ base.py                    # ServiceResult, ErrorCode
â”‚   â”œâ”€â”€ code_loader.py
â”‚   â”œâ”€â”€ analysis.py
â”‚   â”œâ”€â”€ generation.py
â”‚   â”œâ”€â”€ execution.py
â”‚   â”œâ”€â”€ fixing.py
â”‚   â””â”€â”€ github.py
â””â”€â”€ tools/
    â”œâ”€â”€ core/                      # Core MCP tools
    â”‚   â”œâ”€â”€ analyze_code.py
    â”‚   â”œâ”€â”€ generate_tests.py
    â”‚   â”œâ”€â”€ run_tests.py
    â”‚   â”œâ”€â”€ fix_code.py
    â”‚   â”œâ”€â”€ analyzer/              # AST parsing + models
    â”‚   â”œâ”€â”€ generators/            # template + evidence + ai enhancer
    â”‚   â”œâ”€â”€ runner/                # pytest runner + coverage models
    â”‚   â””â”€â”€ fixer/                 # AI fixer + failure parsing
    â””â”€â”€ github/                    # GitHub integration tools (optional)
        â”œâ”€â”€ analyze_repository.py
        â”œâ”€â”€ create_test_pr.py
        â””â”€â”€ comment_test_results.py
```

---

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest -v

# Run with coverage
uv run pytest --cov=src --cov-report=term-missing

# Run a specific file
uv run pytest tests/test_tool_handlers.py -v
```

The test suite includes:
- Unit tests for analyzers, evidence extractors, generators, runner, and services
- Integration tests for full flows (analyze â†’ generate â†’ run)
- GitHub integration tests with mocked API behavior

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `OPENAI_API_KEY` | Enables AI enhancement and AI fixing | Optional |
| `GITHUB_TOKEN` | Enables GitHub integration tools | Optional |

### Key Tool Parameters (high level)

`generate_tests`
- `code` (string) OR `file_path` (string)
- `use_ai` (bool, default: false)
- `include_edge_cases` (bool, default: true)

`run_tests`
- `source_code` (string)
- `test_code` (string)

`fix_code`
- `source_code` (string)
- `test_code` (string)
- `verify` (bool, default: true)

---

## ğŸ”’ Security Notes

This project can execute Python code via `pytest` when using `run_tests`.

- âœ… Intended for trusted code (your own local projects / controlled repos)
- âš ï¸ Do not run untrusted code without sandboxing and stronger resource limits
- Keep `OPENAI_API_KEY` and `GITHUB_TOKEN` in env vars and never commit them

---

## ğŸ—ºï¸ Roadmap (Optional)

- `pipeline` tool: orchestrate analyze â†’ generate â†’ run â†’ fix â†’ rerun in one call (with step-by-step report)
- Mutation testing report (â€œmutation survivorsâ€ + actionable suggestions)
- Stronger sandboxing (timeouts, resource limits, container execution)

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE).

---

## ğŸ™ Acknowledgments

- Model Context Protocol (MCP)
- pytest / pytest-cov
- OpenAI (optional AI enhancement/fixing)
