# ğŸ§ª Pytest Generator MCP Server

> **AI-powered test generation for Python code** â€” Analyze, Generate, Execute

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![MCP Protocol](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)
[![Tests](https://img.shields.io/badge/tests-45%2B%20passing-brightgreen.svg)](#testing)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## âœ¨ What It Does

A complete test automation pipeline as an MCP (Model Context Protocol) server. Works with Claude Desktop, MCP Inspector, and any MCP-compatible client.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE PIPELINE                          â”‚
â”‚                                                               â”‚
â”‚   ğŸ“„ Your Code                                                â”‚
â”‚        â†“                                                      â”‚
â”‚   ğŸ” analyze_code    â†’ Structure, complexity, warnings        â”‚
â”‚        â†“                                                      â”‚
â”‚   ğŸ§ª generate_tests  â†’ Template + AI enhanced tests           â”‚
â”‚        â†“                                                      â”‚
â”‚   â–¶ï¸  run_tests      â†’ Pass/fail results + coverage           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Features

### ğŸ” Code Analysis (`analyze_code`)
- AST-based parsing of Python code
- Extracts functions, classes, methods, parameters
- Calculates cyclomatic complexity
- Reports type hint coverage
- Warns about missing docstrings and high complexity

### ğŸ§ª Test Generation (`generate_tests`)

**Layer 1 â€” Template Tests**
- Basic smoke tests for every function
- Class instantiation tests
- Method invocation tests

**Layer 2 â€” Evidence-Based Enrichment**
- ğŸ“ **Doctest Extraction** â€” Converts docstring examples to assertions
- ğŸ·ï¸ **Type Assertions** â€” Generates `isinstance` checks from type hints
- âš ï¸ **Exception Detection** â€” Creates `pytest.raises` tests from AST
- ğŸ“Š **Boundary Values** â€” Tests edge cases (zero, empty, negative)
- ğŸ”¤ **Naming Heuristics** â€” `is_*` â†’ boolean, `get_*` â†’ returns value

**Layer 3 â€” AI Enhancement** (Optional)
- ğŸ¤– Uses OpenAI to enhance weak assertions
- Replaces `assert result is not None` with `assert result == 5`
- Fixes exception trigger conditions
- Adds meaningful edge case tests
- Falls back to templates if AI unavailable

### â–¶ï¸ Test Execution (`run_tests`)
- Runs tests in isolated temporary environment
- Reports pass/fail with detailed error messages
- Measures actual code coverage via `pytest-cov`
- Auto-detects module name from imports

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.10+
- [uv](https://github.com/astral-sh/uv) (recommended) or pip

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/mcp-pytest-generator.git
cd mcp-pytest-generator

# Install dependencies with uv
uv sync

# Or with pip
pip install -e .
```

### Configure Claude Desktop

Add to `%APPDATA%\Claude\claude_desktop_config.json` (Windows) or `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS):

```json
{
  "mcpServers": {
    "pytest-generator": {
      "command": "C:\\path\\to\\project\\.venv\\Scripts\\python.exe",
      "args": ["-m", "src.server"],
      "cwd": "C:\\path\\to\\project",
      "env": {
        "OPENAI_API_KEY": "sk-your-key-here"
      }
    }
  }
}
```

---

## ğŸ¯ Usage

### With MCP Inspector

```bash
npx @modelcontextprotocol/inspector .venv\Scripts\python.exe -m src.server
```

### With Claude Desktop

Once configured, Claude can use the tools directly:

**"Analyze this code"** â†’ Uses `analyze_code`

**"Generate tests for this code"** â†’ Uses `generate_tests`

**"Generate tests with AI enhancement"** â†’ Uses `generate_tests` with `use_ai=true`

**"Run these tests"** â†’ Uses `run_tests`

---

## ğŸ“– Examples

### Example 1: Simple Function

**Input Code:**
```python
def calculate_discount(price: float, percentage: float) -> float:
    """Calculate discounted price.
    
    >>> calculate_discount(100.0, 10.0)
    90.0
    """
    if percentage < 0 or percentage > 100:
        raise ValueError("Percentage must be between 0 and 100")
    return price * (1 - percentage / 100)
```

**Generated Tests (AI-enhanced):**
```python
def test_calculate_discount_basic():
    assert calculate_discount(100.0, 20.0) == 80.0

def test_calculate_discount_doctest_1():
    assert calculate_discount(100.0, 10.0) == 90.0

def test_calculate_discount_raises_valueerror():
    with pytest.raises(ValueError):
        calculate_discount(100.0, -5.0)
    with pytest.raises(ValueError):
        calculate_discount(100.0, 150.0)

def test_calculate_discount_zero_discount():
    assert calculate_discount(100.0, 0.0) == 100.0

def test_calculate_discount_full_discount():
    assert calculate_discount(100.0, 100.0) == 0.0
```

### Example 2: Run Tests Output

```
ğŸ§ª TEST EXECUTION RESULTS
==================================================

âœ… All tests passed!

ğŸ“Š Summary:
  â€¢ Total:  5
  â€¢ Passed: 5
  â€¢ Failed: 0

ğŸ“ˆ Code Coverage:
  â€¢ Coverage: 100.0%
  â€¢ Lines covered: 6/6

âœ… Passed tests:
  â€¢ test_calculate_discount_basic
  â€¢ test_calculate_discount_doctest_1
  â€¢ test_calculate_discount_raises_valueerror
  â€¢ test_calculate_discount_zero_discount
  â€¢ test_calculate_discount_full_discount
```

---

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ server.py                 # MCP server entry point
â”œâ”€â”€ core/                     # Code analysis engine
â”‚   â”œâ”€â”€ analyzer.py           # Main analyzer
â”‚   â”œâ”€â”€ parser.py             # AST parsing
â”‚   â””â”€â”€ models.py             # Data models
â”œâ”€â”€ generators/               # Test generation
â”‚   â”œâ”€â”€ template_generator.py # Layer 1+2: Templates
â”‚   â”œâ”€â”€ ai_enhancer.py        # Layer 3: AI enhancement
â”‚   â”œâ”€â”€ base.py               # Base classes
â”‚   â””â”€â”€ evidence/             # Evidence extractors
â”‚       â”œâ”€â”€ doctest_extractor.py
â”‚       â”œâ”€â”€ type_assertions.py
â”‚       â”œâ”€â”€ exception_detector.py
â”‚       â””â”€â”€ boundary_values.py
â”œâ”€â”€ runner/                   # Test execution
â”‚   â”œâ”€â”€ executor.py           # Pytest runner
â”‚   â””â”€â”€ models.py             # Result models
â””â”€â”€ validation/               # Code validation
```

### Design Decisions

**Why Hybrid AI + Templates?**

| Templates | AI |
|-----------|-----|
| âœ… 100% accurate (from code) | âœ… Understands logic |
| âœ… Always available | âŒ Requires API key |
| âœ… Free | âŒ Costs money |
| âŒ Can't compute expected values | âœ… Generates real assertions |

**Solution:** Templates provide reliable foundation, AI enhances when available. If AI fails, you still get useful tests.

---

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest -v

# Run with coverage
uv run pytest --cov=src --cov-report=term-missing

# Run specific test file
uv run pytest tests/test_generators.py -v
```

**Test Coverage:**
- Core modules: ~45+ tests
- Generators: Template + AI enhancer tests
- Runner: Execution and coverage tests
- Server: Integration tests

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `OPENAI_API_KEY` | OpenAI API key for AI enhancement | Optional |

### Tool Parameters

**`generate_tests`**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `code` | string | â€” | Python code to test |
| `file_path` | string | â€” | Alternative: path to file |
| `use_ai` | boolean | `false` | Enable AI enhancement |
| `include_edge_cases` | boolean | `true` | Generate boundary tests |

**`run_tests`**
| Parameter | Type | Description |
|-----------|------|-------------|
| `source_code` | string | Python source code |
| `test_code` | string | Pytest test code |

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [Model Context Protocol](https://modelcontextprotocol.io/) by Anthropic
- [OpenAI](https://openai.com/) for AI enhancement capabilities
- [pytest](https://pytest.org/) for the testing framework
